{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMDB Sentiment Analysis Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2020-07-14 16:18:47--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  23.4MB/s    in 4.5s    \n",
      "\n",
      "2020-07-14 16:18:52 (17.7 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_data(filepath = '../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train','test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos','neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(filepath, data_type, sentiment, '*.txt' )\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "    return data,labels                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,labels = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_data(data,labels):\n",
    "    \n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    \n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    return shuffle(data_train,data_test, labels_train,labels_test )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting texthero\n",
      "  Downloading texthero-1.0.9-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: nltk>=3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (3.4.5)\n",
      "Collecting gensim>=3.6.0\n",
      "  Downloading gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2 MB 10.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (4.44.1)\n",
      "Requirement already satisfied: plotly>=4.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (4.8.1)\n",
      "Requirement already satisfied: matplotlib>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (3.1.3)\n",
      "Requirement already satisfied: pandas>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (1.0.3)\n",
      "Collecting unidecode>=1.1.1\n",
      "  Downloading Unidecode-1.1.1-py2.py3-none-any.whl (238 kB)\n",
      "\u001b[K     |████████████████████████████████| 238 kB 43.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wordcloud>=1.5.0\n",
      "  Downloading wordcloud-1.7.0-cp36-cp36m-manylinux1_x86_64.whl (364 kB)\n",
      "\u001b[K     |████████████████████████████████| 364 kB 58.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (1.18.1)\n",
      "Collecting spacy>=2.2.2\n",
      "  Downloading spacy-2.3.2-cp36-cp36m-manylinux1_x86_64.whl (9.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.9 MB 43.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from texthero) (0.22.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk>=3.3->texthero) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from gensim>=3.6.0->texthero) (1.4.1)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-2.1.0.tar.gz (116 kB)\n",
      "\u001b[K     |████████████████████████████████| 116 kB 45.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: retrying>=1.3.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from plotly>=4.2.0->texthero) (1.3.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.1.0->texthero) (1.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.1.0->texthero) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.1.0->texthero) (2.4.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib>=3.1.0->texthero) (0.10.0)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pandas>=1.0.2->texthero) (2019.3)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud>=1.5.0->texthero) (7.0.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (19 kB)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.3-cp36-cp36m-manylinux1_x86_64.whl (32 kB)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->texthero) (46.1.3.post20200330)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->texthero) (2.23.0)\n",
      "Collecting thinc==7.4.1\n",
      "  Downloading thinc-7.4.1-cp36-cp36m-manylinux1_x86_64.whl (2.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.1 MB 71.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.7.0.tar.gz (22 kB)\n",
      "Collecting blis<0.5.0,>=0.4.0\n",
      "  Downloading blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 47.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.2-cp36-cp36m-manylinux1_x86_64.whl (119 kB)\n",
      "\u001b[K     |████████████████████████████████| 119 kB 66.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.2-cp36-cp36m-manylinux1_x86_64.whl (185 kB)\n",
      "\u001b[K     |████████████████████████████████| 185 kB 68.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from scikit-learn>=0.22->texthero) (0.14.1)\n",
      "Requirement already satisfied: boto in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=3.6.0->texthero) (2.49.0)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=3.6.0->texthero) (1.14.16)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (1.5.0)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (1.25.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2020.4.5.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->texthero) (2.9)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (0.9.4)\n",
      "Requirement already satisfied: botocore<1.18.0,>=1.17.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (1.17.16)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (0.3.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->texthero) (2.2.0)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from botocore<1.18.0,>=1.17.16->boto3->smart-open>=1.8.1->gensim>=3.6.0->texthero) (0.15.2)\n",
      "Building wheels for collected packages: smart-open, wasabi\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for smart-open: filename=smart_open-2.1.0-py3-none-any.whl size=110317 sha256=46e21d5975690be5d2f303e1b44c4850a9561c42058328dcc07a07c4f44b4bf6\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/a4/9b/d5/85705a7ab783cd6f7bd718f01d3b1396272f30044e3c36401a\n",
      "  Building wheel for wasabi (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for wasabi: filename=wasabi-0.7.0-py3-none-any.whl size=20832 sha256=0d69dc5e185c594ff6f6d12c05b389cfa3d29f255ebae7f386d08350c2ff99af\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/f3/6f/d7/442c6413b2acf56531cd9e1273047cd565f686b8010353d0aa\n",
      "Successfully built smart-open wasabi\n",
      "Installing collected packages: smart-open, gensim, unidecode, wordcloud, murmurhash, catalogue, cymem, preshed, wasabi, blis, plac, srsly, thinc, spacy, texthero\n",
      "Successfully installed blis-0.4.1 catalogue-1.0.0 cymem-2.0.3 gensim-3.8.3 murmurhash-1.0.2 plac-1.1.3 preshed-3.0.2 smart-open-2.1.0 spacy-2.3.2 srsly-1.0.2 texthero-1.0.9 thinc-7.4.1 unidecode-1.1.1 wasabi-0.7.0 wordcloud-1.7.0\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.1.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/python3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#Trying out TEXTHERO : https://texthero.org/docs/getting-started\n",
    "!pip install texthero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the second alternate gundam universe tale g gundam being the first gundam wing is yet another different view into the gundam verse the familiar elements are found but gundam wing is actually different then its counterparts the biggest being the gundams are nothing more than terrorists combating one lone organization in truth the series doesnt really become a show about war until episode 7 but in truth the real conflict the eve wars dont happen until the later episodes  the greatest positives of this series are its characters all the main characters are fleshed out throughout the 49 episode run and you can really sympathize with each of the roles their put in another great plus is the fantastic character and mecha design of the series the designs put some of its other gundam counterparts to shame  one of the biggest criticism of this series is how many die hard uc fans claim rip off of the original uc saga why gundam wing gets this rap when the more apparent uc clone of gundam seed is out there is beyond me true there are many moments lifted but their told in new ways and there are distinct differences as well take for example the usual comparison of zechs merquise and uc icon char aznable throughout the series zechs is more the outcast in the alliance and in ways oz as well while the red comet was shining symbol of zeon another big difference is the fact zechs loses a lot soldiers under his command hence the other nickname hes given in the early episodes killer of his own men char isnt given this label  the problems with this series isnt the philosophy mumbo jumbo but two problems the first is the reused animation footage of the gundams attacks sure its fun seeing heavy arms attack tanks ms and planes the first time but on its fourth re use scenes like this do get old  the second problem is that the entire series is supposed to take place during an entire year if you really think about all the events springing in the series a lot happens in just one lone year  but i guess you can easily dismiss this fact when ignoring the intros first lines every so often as it ranks this is probably the best of the alternate universe gundam tales and a great introduction into the gundam world after all this was the very first gundam anime to air in the us television'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = os.path.join(\"../cache\", \"preprocessed_data\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def preprocess_data(data_train,data_test, labels_train,labels_test,cache_file=\"preprocessed_data.pkl\" ):\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "   \n",
    "    if cache_data is None:\n",
    "        \n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote preprocessed data to cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Bag-of-Words features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning:\n",
      "\n",
      "sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote features to cache file: bow_features.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)\n",
    "len(train_X[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The __XGBoost classifier__ that we will be using requires the dataset to be written to a file and stored using Amazon S3. To do this, we will start by splitting the training dataset into two parts, the data we will train the model with and a validation set. Then, we will write those datasets to a file locally and then upload the files to S3. In addition, we will write the test set to a file and upload that file to S3. This is so that we can use SageMakers Batch Transform functionality to test our model once we've fit it.\n",
    "The documentation for the XGBoost algorithm in SageMaker requires that the training and validation datasets should contain no headers or index and that the label should occur first for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../'\n",
    "pd.DataFrame(test_X).to_csv(os.path.join(data_dir, 'test.csv'), header=False, index=False)\n",
    "\n",
    "pd.concat([val_y, val_X], axis=1).to_csv(os.path.join(data_dir, 'validation.csv'), header=False, index=False)\n",
    "pd.concat([train_y, train_X], axis=1).to_csv(os.path.join(data_dir, 'train.csv'), header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def upload_to_s3(file_to_upload,file_in_s3):\n",
    "    bucket='exercisedatasj' \n",
    "    prefix = f'sagemaker/sentiment_analysis/{file_in_s3}' \n",
    "\n",
    "\n",
    "    key = \"{}\".format(prefix)\n",
    "\n",
    "    url = 's3://{}/{}'.format(bucket, key)\n",
    "    boto3.Session().resource('s3').Bucket(bucket).Object(key).upload_file(file_to_upload)\n",
    "    print('Done writing to {}'.format(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done writing to s3://exercisedatasj/sagemaker/sentiment_analysis/test.csv\n",
      "Done writing to s3://exercisedatasj/sagemaker/sentiment_analysis/validation.csv\n",
      "Done writing to s3://exercisedatasj/sagemaker/sentiment_analysis/train.csv\n"
     ]
    }
   ],
   "source": [
    "upload_to_s3(os.path.join(data_dir, 'test.csv'),'test.csv')\n",
    "upload_to_s3(os.path.join(data_dir, 'validation.csv'),'validation.csv')\n",
    "upload_to_s3(os.path.join(data_dir, 'train.csv'),'train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating XGBoost Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "session = sagemaker.Session() # Store the current SageMaker session\n",
    "\n",
    "# Our current execution role is required when creating the model as the training\n",
    "# and inference code will need to access the model artifacts.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'get_image_uri' method will be deprecated in favor of 'ImageURIProvider' class in SageMaker Python SDK v2.\n",
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='1.0-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '1.0-1').\n"
     ]
    }
   ],
   "source": [
    "# We need to retrieve the location of the container which is provided by Amazon for using XGBoost.\n",
    "# As a matter of convenience, the training and inference code both use the same container.\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "container = get_image_uri(session.boto_region_name, 'xgboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "# First we create a SageMaker estimator object for our model.\n",
    "xgb = sagemaker.estimator.Estimator(container, # The location of the container we wish to use\n",
    "                                    role,                                    # What is our current IAM Role\n",
    "                                    train_instance_count=1,                  # How many compute instances\n",
    "                                    train_instance_type='ml.m4.xlarge',      # What kind of compute instances\n",
    "                                    output_path='s3://{}/{}/output'.format(session.default_bucket(), prefix),\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "# And then set the algorithm specific parameters.\n",
    "xgb.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        early_stopping_rounds=10,\n",
    "                        num_round=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:'s3_input' class will be renamed to 'TrainingInput' in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://exercisedatasj/sagemaker/sentiment_analysis/train.csv', content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://exercisedatasj/sagemaker/sentiment_analysis/validation.csv', content_type='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-07-14 17:30:07 Starting - Starting the training job...\n",
      "2020-07-14 17:30:10 Starting - Launching requested ML instances......\n",
      "2020-07-14 17:31:29 Starting - Preparing the instances for training......\n",
      "2020-07-14 17:32:28 Downloading - Downloading input data...\n",
      "2020-07-14 17:32:43 Training - Downloading the training image.\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m[2020-07-14:17:33:10:INFO] Running standalone xgboost training.\u001b[0m\n",
      "\u001b[34m[2020-07-14:17:33:10:INFO] File size need to be processed in the node: 238.5mb. Available memory size in the node: 8506.81mb\u001b[0m\n",
      "\u001b[34m[2020-07-14:17:33:10:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[17:33:10] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[17:33:12] 15000x5000 matrix with 75000000 entries loaded from /opt/ml/input/data/train?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[2020-07-14:17:33:12:INFO] Determined delimiter of CSV input is ','\u001b[0m\n",
      "\u001b[34m[17:33:12] S3DistributionType set as FullyReplicated\u001b[0m\n",
      "\u001b[34m[17:33:13] 10000x5000 matrix with 50000000 entries loaded from /opt/ml/input/data/validation?format=csv&label_column=0&delimiter=,\u001b[0m\n",
      "\u001b[34m[17:33:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[0]#011train-error:0.3034#011validation-error:0.313\u001b[0m\n",
      "\u001b[34mMultiple eval metrics have been passed: 'validation-error' will be used for early stopping.\n",
      "\u001b[0m\n",
      "\u001b[34mWill train until validation-error hasn't improved in 10 rounds.\u001b[0m\n",
      "\u001b[34m[17:33:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[1]#011train-error:0.290267#011validation-error:0.2978\u001b[0m\n",
      "\u001b[34m[17:33:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[2]#011train-error:0.275#011validation-error:0.2839\u001b[0m\n",
      "\u001b[34m[17:33:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\n",
      "2020-07-14 17:33:10 Training - Training image download completed. Training in progress.\u001b[34m[3]#011train-error:0.272667#011validation-error:0.2803\u001b[0m\n",
      "\u001b[34m[17:33:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[4]#011train-error:0.271667#011validation-error:0.2783\u001b[0m\n",
      "\u001b[34m[17:33:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[5]#011train-error:0.260333#011validation-error:0.2639\u001b[0m\n",
      "\u001b[34m[17:33:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[6]#011train-error:0.2528#011validation-error:0.2602\u001b[0m\n",
      "\u001b[34m[17:33:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[7]#011train-error:0.246067#011validation-error:0.2532\u001b[0m\n",
      "\u001b[34m[17:33:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[8]#011train-error:0.239133#011validation-error:0.2455\u001b[0m\n",
      "\u001b[34m[17:33:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[9]#011train-error:0.231667#011validation-error:0.24\u001b[0m\n",
      "\u001b[34m[17:33:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[10]#011train-error:0.225333#011validation-error:0.2349\u001b[0m\n",
      "\u001b[34m[17:33:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[11]#011train-error:0.221#011validation-error:0.2352\u001b[0m\n",
      "\u001b[34m[17:33:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[12]#011train-error:0.217267#011validation-error:0.2326\u001b[0m\n",
      "\u001b[34m[17:33:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[13]#011train-error:0.2142#011validation-error:0.2278\u001b[0m\n",
      "\u001b[34m[17:33:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[14]#011train-error:0.2092#011validation-error:0.2223\u001b[0m\n",
      "\u001b[34m[17:33:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[15]#011train-error:0.203133#011validation-error:0.2191\u001b[0m\n",
      "\u001b[34m[17:33:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[16]#011train-error:0.2022#011validation-error:0.2159\u001b[0m\n",
      "\u001b[34m[17:33:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[17]#011train-error:0.198467#011validation-error:0.2154\u001b[0m\n",
      "\u001b[34m[17:33:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[18]#011train-error:0.196#011validation-error:0.2128\u001b[0m\n",
      "\u001b[34m[17:33:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[19]#011train-error:0.193333#011validation-error:0.2104\u001b[0m\n",
      "\u001b[34m[17:33:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[20]#011train-error:0.192#011validation-error:0.2098\u001b[0m\n",
      "\u001b[34m[17:33:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[21]#011train-error:0.189467#011validation-error:0.2092\u001b[0m\n",
      "\u001b[34m[17:33:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[22]#011train-error:0.186667#011validation-error:0.2085\u001b[0m\n",
      "\u001b[34m[17:33:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[23]#011train-error:0.184467#011validation-error:0.2052\u001b[0m\n",
      "\u001b[34m[17:33:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[24]#011train-error:0.1822#011validation-error:0.2028\u001b[0m\n",
      "\u001b[34m[17:33:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[25]#011train-error:0.1792#011validation-error:0.2015\u001b[0m\n",
      "\u001b[34m[17:33:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[26]#011train-error:0.1778#011validation-error:0.2002\u001b[0m\n",
      "\u001b[34m[17:33:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[27]#011train-error:0.176133#011validation-error:0.1982\u001b[0m\n",
      "\u001b[34m[17:33:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 16 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[28]#011train-error:0.1754#011validation-error:0.1968\u001b[0m\n",
      "\u001b[34m[17:33:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[29]#011train-error:0.171867#011validation-error:0.1949\u001b[0m\n",
      "\u001b[34m[17:33:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[30]#011train-error:0.168333#011validation-error:0.1936\u001b[0m\n",
      "\u001b[34m[17:33:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 18 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[31]#011train-error:0.166733#011validation-error:0.1915\u001b[0m\n",
      "\u001b[34m[17:33:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[32]#011train-error:0.164#011validation-error:0.1909\u001b[0m\n",
      "\u001b[34m[17:33:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[33]#011train-error:0.162733#011validation-error:0.1893\u001b[0m\n",
      "\u001b[34m[17:34:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[34]#011train-error:0.161533#011validation-error:0.1883\u001b[0m\n",
      "\u001b[34m[17:34:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[35]#011train-error:0.1594#011validation-error:0.1881\u001b[0m\n",
      "\u001b[34m[17:34:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[36]#011train-error:0.157867#011validation-error:0.1851\u001b[0m\n",
      "\u001b[34m[17:34:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[37]#011train-error:0.1574#011validation-error:0.1852\u001b[0m\n",
      "\u001b[34m[17:34:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[38]#011train-error:0.1554#011validation-error:0.1849\u001b[0m\n",
      "\u001b[34m[17:34:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[39]#011train-error:0.154267#011validation-error:0.1838\u001b[0m\n",
      "\u001b[34m[17:34:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[40]#011train-error:0.1528#011validation-error:0.184\u001b[0m\n",
      "\u001b[34m[17:34:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[41]#011train-error:0.151#011validation-error:0.1837\u001b[0m\n",
      "\u001b[34m[17:34:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[42]#011train-error:0.1494#011validation-error:0.18\u001b[0m\n",
      "\u001b[34m[17:34:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[43]#011train-error:0.147933#011validation-error:0.1787\u001b[0m\n",
      "\u001b[34m[17:34:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[44]#011train-error:0.147267#011validation-error:0.1786\u001b[0m\n",
      "\u001b[34m[17:34:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[45]#011train-error:0.146267#011validation-error:0.1774\u001b[0m\n",
      "\u001b[34m[17:34:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[46]#011train-error:0.145333#011validation-error:0.1773\u001b[0m\n",
      "\u001b[34m[17:34:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[47]#011train-error:0.144733#011validation-error:0.1754\u001b[0m\n",
      "\u001b[34m[17:34:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[48]#011train-error:0.1442#011validation-error:0.1761\u001b[0m\n",
      "\u001b[34m[17:34:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[49]#011train-error:0.1428#011validation-error:0.1747\u001b[0m\n",
      "\u001b[34m[17:34:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[50]#011train-error:0.141867#011validation-error:0.1745\u001b[0m\n",
      "\u001b[34m[17:34:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[51]#011train-error:0.139067#011validation-error:0.1728\u001b[0m\n",
      "\u001b[34m[17:34:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[52]#011train-error:0.1372#011validation-error:0.171\u001b[0m\n",
      "\u001b[34m[17:34:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[53]#011train-error:0.136333#011validation-error:0.1716\u001b[0m\n",
      "\u001b[34m[17:34:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[54]#011train-error:0.135267#011validation-error:0.1714\u001b[0m\n",
      "\u001b[34m[17:34:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[55]#011train-error:0.134467#011validation-error:0.1702\u001b[0m\n",
      "\u001b[34m[17:34:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[56]#011train-error:0.134133#011validation-error:0.1683\u001b[0m\n",
      "\u001b[34m[17:34:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[57]#011train-error:0.133267#011validation-error:0.1681\u001b[0m\n",
      "\u001b[34m[17:34:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[58]#011train-error:0.1322#011validation-error:0.1678\u001b[0m\n",
      "\u001b[34m[17:34:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[59]#011train-error:0.131#011validation-error:0.1678\u001b[0m\n",
      "\u001b[34m[17:34:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[60]#011train-error:0.130533#011validation-error:0.1665\u001b[0m\n",
      "\u001b[34m[17:34:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[61]#011train-error:0.129667#011validation-error:0.1674\u001b[0m\n",
      "\u001b[34m[17:34:36] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[62]#011train-error:0.128467#011validation-error:0.1669\u001b[0m\n",
      "\u001b[34m[17:34:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[63]#011train-error:0.1276#011validation-error:0.1646\u001b[0m\n",
      "\u001b[34m[17:34:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[64]#011train-error:0.126733#011validation-error:0.1652\u001b[0m\n",
      "\u001b[34m[17:34:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[65]#011train-error:0.125733#011validation-error:0.1646\u001b[0m\n",
      "\u001b[34m[17:34:41] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[66]#011train-error:0.125067#011validation-error:0.1639\u001b[0m\n",
      "\u001b[34m[17:34:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[67]#011train-error:0.1236#011validation-error:0.1647\u001b[0m\n",
      "\u001b[34m[17:34:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[68]#011train-error:0.1216#011validation-error:0.1627\u001b[0m\n",
      "\u001b[34m[17:34:45] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[69]#011train-error:0.121133#011validation-error:0.162\u001b[0m\n",
      "\u001b[34m[17:34:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[70]#011train-error:0.12#011validation-error:0.1618\u001b[0m\n",
      "\u001b[34m[17:34:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[71]#011train-error:0.1194#011validation-error:0.1638\u001b[0m\n",
      "\u001b[34m[17:34:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[72]#011train-error:0.119067#011validation-error:0.1625\u001b[0m\n",
      "\u001b[34m[17:34:50] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[73]#011train-error:0.1176#011validation-error:0.1618\u001b[0m\n",
      "\u001b[34m[17:34:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[74]#011train-error:0.117133#011validation-error:0.16\u001b[0m\n",
      "\u001b[34m[17:34:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[75]#011train-error:0.116733#011validation-error:0.1587\u001b[0m\n",
      "\u001b[34m[17:34:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[76]#011train-error:0.1156#011validation-error:0.1588\u001b[0m\n",
      "\u001b[34m[17:34:55] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[77]#011train-error:0.1158#011validation-error:0.1585\u001b[0m\n",
      "\u001b[34m[17:34:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[78]#011train-error:0.115333#011validation-error:0.1584\u001b[0m\n",
      "\u001b[34m[17:34:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 26 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[79]#011train-error:0.114333#011validation-error:0.1573\u001b[0m\n",
      "\u001b[34m[17:34:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[80]#011train-error:0.114133#011validation-error:0.1565\u001b[0m\n",
      "\u001b[34m[17:35:00] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[81]#011train-error:0.1134#011validation-error:0.1557\u001b[0m\n",
      "\u001b[34m[17:35:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[82]#011train-error:0.1124#011validation-error:0.1548\u001b[0m\n",
      "\u001b[34m[17:35:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[83]#011train-error:0.111133#011validation-error:0.1545\u001b[0m\n",
      "\u001b[34m[17:35:04] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[84]#011train-error:0.1106#011validation-error:0.1548\u001b[0m\n",
      "\u001b[34m[17:35:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[85]#011train-error:0.110533#011validation-error:0.1531\u001b[0m\n",
      "\u001b[34m[17:35:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[86]#011train-error:0.1102#011validation-error:0.1521\u001b[0m\n",
      "\u001b[34m[17:35:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[87]#011train-error:0.109933#011validation-error:0.1523\u001b[0m\n",
      "\u001b[34m[17:35:09] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[88]#011train-error:0.1092#011validation-error:0.1528\u001b[0m\n",
      "\u001b[34m[17:35:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 28 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[89]#011train-error:0.107533#011validation-error:0.1534\u001b[0m\n",
      "\u001b[34m[17:35:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[90]#011train-error:0.1068#011validation-error:0.1536\u001b[0m\n",
      "\u001b[34m[17:35:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[91]#011train-error:0.1056#011validation-error:0.1524\u001b[0m\n",
      "\u001b[34m[17:35:14] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[92]#011train-error:0.104667#011validation-error:0.151\u001b[0m\n",
      "\u001b[34m[17:35:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[93]#011train-error:0.104667#011validation-error:0.1512\u001b[0m\n",
      "\u001b[34m[17:35:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 24 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[94]#011train-error:0.104867#011validation-error:0.1499\u001b[0m\n",
      "\u001b[34m[17:35:18] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[95]#011train-error:0.104067#011validation-error:0.15\u001b[0m\n",
      "\u001b[34m[17:35:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[96]#011train-error:0.1034#011validation-error:0.1501\u001b[0m\n",
      "\u001b[34m[17:35:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 20 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[97]#011train-error:0.102933#011validation-error:0.1501\u001b[0m\n",
      "\u001b[34m[17:35:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[98]#011train-error:0.1024#011validation-error:0.1501\u001b[0m\n",
      "\u001b[34m[17:35:23] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[99]#011train-error:0.1022#011validation-error:0.1492\u001b[0m\n",
      "\u001b[34m[17:35:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[100]#011train-error:0.102533#011validation-error:0.15\u001b[0m\n",
      "\u001b[34m[17:35:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[101]#011train-error:0.102133#011validation-error:0.1497\u001b[0m\n",
      "\u001b[34m[17:35:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[102]#011train-error:0.1008#011validation-error:0.1497\u001b[0m\n",
      "\u001b[34m[17:35:28] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 0 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[103]#011train-error:0.1004#011validation-error:0.1501\u001b[0m\n",
      "\u001b[34m[17:35:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[104]#011train-error:0.100933#011validation-error:0.1493\u001b[0m\n",
      "\u001b[34m[17:35:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[105]#011train-error:0.1006#011validation-error:0.1486\u001b[0m\n",
      "\u001b[34m[17:35:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[106]#011train-error:0.100733#011validation-error:0.1493\u001b[0m\n",
      "\u001b[34m[17:35:33] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[107]#011train-error:0.099667#011validation-error:0.1492\u001b[0m\n",
      "\u001b[34m[17:35:34] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[108]#011train-error:0.0994#011validation-error:0.1489\u001b[0m\n",
      "\u001b[34m[17:35:35] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[109]#011train-error:0.097733#011validation-error:0.1487\u001b[0m\n",
      "\u001b[34m[17:35:37] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[110]#011train-error:0.098133#011validation-error:0.1484\u001b[0m\n",
      "\u001b[34m[17:35:38] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[111]#011train-error:0.096733#011validation-error:0.1482\u001b[0m\n",
      "\u001b[34m[17:35:39] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[112]#011train-error:0.096133#011validation-error:0.1484\u001b[0m\n",
      "\u001b[34m[17:35:40] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[113]#011train-error:0.0958#011validation-error:0.1481\u001b[0m\n",
      "\u001b[34m[17:35:42] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[114]#011train-error:0.0952#011validation-error:0.1483\u001b[0m\n",
      "\u001b[34m[17:35:43] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[115]#011train-error:0.094867#011validation-error:0.1481\u001b[0m\n",
      "\u001b[34m[17:35:44] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[116]#011train-error:0.0948#011validation-error:0.1486\u001b[0m\n",
      "\u001b[34m[17:35:46] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[117]#011train-error:0.094333#011validation-error:0.1492\u001b[0m\n",
      "\u001b[34m[17:35:47] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[118]#011train-error:0.093667#011validation-error:0.1486\u001b[0m\n",
      "\u001b[34m[17:35:48] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[119]#011train-error:0.093067#011validation-error:0.1484\u001b[0m\n",
      "\u001b[34m[17:35:49] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[120]#011train-error:0.092933#011validation-error:0.1486\u001b[0m\n",
      "\u001b[34m[17:35:51] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[121]#011train-error:0.093133#011validation-error:0.1483\u001b[0m\n",
      "\u001b[34m[17:35:52] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[122]#011train-error:0.093267#011validation-error:0.1485\u001b[0m\n",
      "\u001b[34m[17:35:53] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[123]#011train-error:0.0934#011validation-error:0.1478\u001b[0m\n",
      "\u001b[34m[17:35:54] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[124]#011train-error:0.092533#011validation-error:0.147\u001b[0m\n",
      "\u001b[34m[17:35:56] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 14 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[125]#011train-error:0.092733#011validation-error:0.1472\u001b[0m\n",
      "\u001b[34m[17:35:57] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[126]#011train-error:0.092733#011validation-error:0.1478\u001b[0m\n",
      "\u001b[34m[17:35:58] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[127]#011train-error:0.092267#011validation-error:0.147\u001b[0m\n",
      "\u001b[34m[17:35:59] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[128]#011train-error:0.091267#011validation-error:0.1469\u001b[0m\n",
      "\u001b[34m[17:36:01] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[129]#011train-error:0.090467#011validation-error:0.1466\u001b[0m\n",
      "\u001b[34m[17:36:02] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[130]#011train-error:0.090467#011validation-error:0.1463\u001b[0m\n",
      "\u001b[34m[17:36:03] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[131]#011train-error:0.0904#011validation-error:0.145\u001b[0m\n",
      "\u001b[34m[17:36:05] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[132]#011train-error:0.0898#011validation-error:0.1448\u001b[0m\n",
      "\u001b[34m[17:36:06] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[133]#011train-error:0.089667#011validation-error:0.1458\u001b[0m\n",
      "\u001b[34m[17:36:07] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[134]#011train-error:0.0892#011validation-error:0.145\u001b[0m\n",
      "\u001b[34m[17:36:08] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[135]#011train-error:0.0888#011validation-error:0.1447\u001b[0m\n",
      "\u001b[34m[17:36:10] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[136]#011train-error:0.088267#011validation-error:0.1442\u001b[0m\n",
      "\u001b[34m[17:36:11] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[137]#011train-error:0.087933#011validation-error:0.1439\u001b[0m\n",
      "\u001b[34m[17:36:12] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[138]#011train-error:0.0876#011validation-error:0.1431\u001b[0m\n",
      "\u001b[34m[17:36:13] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[139]#011train-error:0.087133#011validation-error:0.143\u001b[0m\n",
      "\u001b[34m[17:36:15] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 16 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[140]#011train-error:0.086733#011validation-error:0.1428\u001b[0m\n",
      "\u001b[34m[17:36:16] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 6 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[141]#011train-error:0.087#011validation-error:0.1421\u001b[0m\n",
      "\u001b[34m[17:36:17] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[142]#011train-error:0.0872#011validation-error:0.142\u001b[0m\n",
      "\u001b[34m[17:36:19] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[143]#011train-error:0.087067#011validation-error:0.1419\u001b[0m\n",
      "\u001b[34m[17:36:20] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[144]#011train-error:0.086467#011validation-error:0.1411\u001b[0m\n",
      "\u001b[34m[17:36:21] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[145]#011train-error:0.086533#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[17:36:22] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[146]#011train-error:0.086133#011validation-error:0.1419\u001b[0m\n",
      "\u001b[34m[17:36:24] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 10 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[147]#011train-error:0.0856#011validation-error:0.1414\u001b[0m\n",
      "\u001b[34m[17:36:25] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[148]#011train-error:0.085267#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[17:36:26] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 12 extra nodes, 4 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[149]#011train-error:0.085533#011validation-error:0.1413\u001b[0m\n",
      "\u001b[34m[17:36:27] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[150]#011train-error:0.085067#011validation-error:0.1413\u001b[0m\n",
      "\n",
      "2020-07-14 17:36:38 Uploading - Uploading generated training model\u001b[34m[17:36:29] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 14 extra nodes, 2 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[151]#011train-error:0.085#011validation-error:0.1417\u001b[0m\n",
      "\u001b[34m[17:36:30] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 18 extra nodes, 10 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[152]#011train-error:0.084533#011validation-error:0.1415\u001b[0m\n",
      "\u001b[34m[17:36:31] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 20 extra nodes, 8 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[153]#011train-error:0.084067#011validation-error:0.1416\u001b[0m\n",
      "\u001b[34m[17:36:32] src/tree/updater_prune.cc:74: tree pruning end, 1 roots, 22 extra nodes, 12 pruned nodes, max_depth=5\u001b[0m\n",
      "\u001b[34m[154]#011train-error:0.083667#011validation-error:0.142\u001b[0m\n",
      "\u001b[34mStopping. Best iteration:\u001b[0m\n",
      "\u001b[34m[144]#011train-error:0.086467#011validation-error:0.1411\n",
      "\u001b[0m\n",
      "\n",
      "2020-07-14 17:36:45 Completed - Training job completed\n",
      "Training seconds: 257\n",
      "Billable seconds: 257\n"
     ]
    }
   ],
   "source": [
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model with Batch Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = train_X = val_X = train_y = val_y = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "xgb_transformer = xgb.transformer(instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_transformer.transform('s3://exercisedatasj/sagemaker/sentiment_analysis/test.csv', content_type='text/csv', split_type='Line')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Now the transform job has executed and the result, the estimated sentiment of each review, has been saved on S3. Since we would rather work on this file locally we can perform a bit of notebook magic to copy the file to the data_dir__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !aws s3 cp --recursive $xgb_transformer.output_path $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-577153973786/xgboost-2020-07-14-17-37-22-327'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_transformer.output_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pd.read_csv(os.path.join(xgb_transformer.output_path,'test.csv.out'),header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.011025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.054424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.861829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.178202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.125888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>0.460890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>0.947942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>0.798385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>0.614617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>0.022842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0\n",
       "0      0.011025\n",
       "1      0.054424\n",
       "2      0.861829\n",
       "3      0.178202\n",
       "4      0.125888\n",
       "...         ...\n",
       "24995  0.460890\n",
       "24996  0.947942\n",
       "24997  0.798385\n",
       "24998  0.614617\n",
       "24999  0.022842\n",
       "\n",
       "[25000 rows x 1 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [round(num) for num in predictions.squeeze().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8584"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(test_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker:Parameter image will be renamed to image_uri in SageMaker Python SDK v2.\n",
      "WARNING:sagemaker:Using already existing model: xgboost-2020-07-14-17-30-07-049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, instance_type = 'ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "runtime = boto3.Session().client('sagemaker-runtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xgboost-2020-07-14-17-30-07-049'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_predictor.endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = \"Nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies, totally devoid of any heart or heat. A romantic comedy that has zero romantic chemestry and zero laughs!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies totally devoid of any heart or heat a romantic comedy that has zero romantic chemestry and zero laughs\n"
     ]
    }
   ],
   "source": [
    "test_words = review_to_words(test_review)\n",
    "print(test_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_encoding(words, vocabulary):\n",
    "    bow = [0] * len(vocabulary) # Start by setting the count for each word in the vocabulary to zero.\n",
    "    for word in words.split():  # For each word in the string\n",
    "        if word in vocabulary:  # If the word is one that occurs in the vocabulary, increase its count.\n",
    "            bow[vocabulary[word]] += 1\n",
    "    return bow\n",
    "\n",
    "test_bow = bow_encoding(test_words, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.Session().client('sagemaker-runtime')\n",
    "\n",
    "response = runtime.invoke_endpoint(EndpointName = xgb_predictor.endpoint, # The name of the endpoint we created\n",
    "                                       ContentType = 'text/csv',                     # The data format that is expected\n",
    "                                       Body = ','.join([str(val) for val in test_bow]).encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.399025201797\n"
     ]
    }
   ],
   "source": [
    "response = response['Body'].read().decode('utf-8')\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
